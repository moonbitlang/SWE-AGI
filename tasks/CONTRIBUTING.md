# Contributing Spec-Driven Test Suites

Thanks for contributing a new spec-driven test suite. This guide focuses on how
to design a suite that is derived from a formal specification and usable by both
humans and LLMs.

## Required components

### specs/ folder

Each suite must include a `specs/` directory with the specification materials
the tests are based on. This folder is the normative source for the suite.

### TASK.md

Each suite must include a `TASK.md` that instructs an LLM how to implement a
spec-compliant solution. This is the single entry point for agents and models.

### Test cases

Each suite must include test cases derived from official test cases, de-facto
implementations, or LLM-generated cases that are verified by a human. The source
information should be recorded faithfully in the `README.md` of this test suite.
Tests must be categorized into happy path, error path, and any other relevant
categories.

## Directory layout example

```
<spec>/
  specs/
    README.md
    <spec-sources>
  TASK.md
  <spec>_spec.mbt
  <spec>_pub_test.mbt
  <spec>_priv_test.mbt
  README.md
  moon.mod.json
  moon.pkg.json
```

## Writing the specs/ folder

1. Include the official specification or an excerpt that covers every test case
   in the suite.
2. Add a `specs/README.md` with source URLs, version numbers, and any licensing
   notes.
3. Make clear what is normative vs. informative material if the source document
   distinguishes them.
4. If you must rely on a de-facto behavior not specified, call it out explicitly
   in `specs/README.md`.
5. Keep specs in simple formats when possible (txt, md, pdf). Avoid
   autogenerated or huge binaries.

## Writing TASK.md

TASK.md should be readable by humans and unambiguous for LLMs. Include at least
the following sections.

1. Goal: what the implementation must do, in spec terms.
2. Scope: what is in scope and explicitly out of scope.
3. Required APIs: functions, types, modules, and names the implementation must
   provide.
4. Behavior rules: parsing, normalization, errors, and any required edge
   behavior.
5. Inputs and outputs: supported formats, encoding rules, and what is considered
   invalid.
6. Test execution: how to run tests, and what a passing run looks like.
7. Non-goals and constraints: no network, deterministic behavior, and any
   performance limits.

Use concrete language. Avoid vague terms like "should handle most cases." If
there are degrees of compliance, state the exact level expected in tests.

## Building test cases

### Sources and provenance

1. Official tests: cite the upstream suite and version, and note any
   transformations.
2. De-facto implementations: record the implementation name and version, and why
   its behavior is treated as canonical.
3. LLM-generated tests: require human verification and record who verified them
   and how.

Store provenance in the test file header comment or the suite README.

### Volume guidance

For a project of moderate complexity, aim for more than 1,000 total test cases
across all categories.

### Categories

Every suite must include at least:

1. Happy path: valid inputs that exercise core features.
2. Error path: invalid inputs and required error behavior.

Recommended additional categories when applicable:

1. Edge cases: boundary sizes, empty inputs, maximum nesting, or limits from the
   spec.
2. Ambiguities: inputs where the spec allows multiple interpretations; tests
   should pin down the chosen interpretation.
3. Compatibility: behaviors that match real-world data or dominant
   implementations.
4. Regression: bugs fixed in prior work or known pitfalls from existing parsers.

### Quality bar

1. Keep tests deterministic and stable across platforms.
2. Avoid network, file system side effects, or nondeterministic randomness.
3. Make each test minimal while still proving the behavior.
4. Include explicit expected results for both success and failure cases.

## Test Organization and Regrouping

Tests may be split across multiple files for maintainability. This repository
may also regroup or reorganize tests over time. To make that safe:

1. Keep tests independent from one another and avoid order dependence.
2. Do not rely on shared mutable state across test files.
3. Use clear naming or comments for especially complex or rare behaviors.

## Submission checklist

1. `specs/` is complete and has a `README.md` with sources and versions.
2. `TASK.md` is precise and runnable by an LLM without additional context.
3. Tests are categorized into happy path, error path, and other relevant groups.
4. Provenance is documented for all non-trivial test cases.
5. `moon test` passes from the suite directory.
