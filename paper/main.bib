@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{larsson2016fractalnet,
  title={Fractalnet: Ultra-deep neural networks without residuals},
  author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
  journal={arXiv preprint arXiv:1605.07648},
  year={2016}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@inproceedings{chollet2017xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}

@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and De Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019}
}



@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@inproceedings{yu2018deep,
  title={Deep layer aggregation},
  author={Yu, Fisher and Wang, Dequan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2403--2412},
  year={2018}
}

@article{zhu2024hyper,
  title={Hyper-connections},
  author={Zhu, Defa and Huang, Hongzhi and Huang, Zihao and Zeng, Yutao and Mao, Yunyao and Wu, Banggu and Min, Qiyang and Zhou, Xun},
  journal={arXiv preprint arXiv:2409.19606},
  year={2024}
}

@article{mak2025residual,
  title={Residual Matrix Transformers: Scaling the Size of the Residual Stream},
  author={Mak, Brian and Flanigan, Jeffrey},
  journal={arXiv preprint arXiv:2506.22696},
  year={2025}
}

@article{xiao2025muddformer,
  title={Muddformer: Breaking residual bottlenecks in transformers via multiway dynamic dense connections},
  author={Xiao, Da and Meng, Qingye and Li, Shengping and Yuan, Xingyuan},
  journal={arXiv preprint arXiv:2502.12170},
  year={2025}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wang2024auxiliary,
  title={Auxiliary-loss-free load balancing strategy for mixture-of-experts},
  author={Wang, Lean and Gao, Huazuo and Zhao, Chenggang and Sun, Xu and Dai, Damai},
  journal={arXiv preprint arXiv:2408.15664},
  year={2024}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}



@article{wang2025tilelang,
  title={TileLang: A Composable Tiled Programming Model for AI Systems},
  author={Wang, Lei and Cheng, Yu and Shi, Yining and Tang, Zhengju and Mo, Zhiwen and Xie, Wenhao and Ma, Lingxiao and Xia, Yuqing and Xue, Jilong and Yang, Fan and others},
  journal={arXiv preprint arXiv:2504.17577},
  year={2025}
}

@inproceedings{Hoffmann2022Chinchilla,
 author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Thomas and Noland, Eric and Millican, Katherine and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Kar\'{e}n and Elsen, Erich and Vinyals, Oriol and Rae, Jack and Sifre, Laurent},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {30016--30030},
 publisher = {Curran Associates, Inc.},
 title = {An empirical analysis of compute-optimal large language model training},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@inproceedings{
qi2024zero,
title={Zero Bubble (Almost) Pipeline Parallelism},
author={Penghui Qi and Xinyi Wan and Guangxing Huang and Min Lin},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tuzTN0eIO5}
}

@inproceedings{
pagliardini2024denseformer,
title={DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging},
author={Matteo Pagliardini and Amirkeivan Mohtashami and Fran{\c{c}}ois Fleuret and Martin Jaggi},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=kMnoh7CXrq}
}

@inproceedings{
menghani2025laurel,
title={{LA}uReL: Learned Augmented Residual Layer},
author={Gaurav Menghani and Ravi Kumar and Sanjiv Kumar},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=rUDRWP9WvZ}
}

@inproceedings{
heddes2025deepcrossattention,
title={DeepCrossAttention: Supercharging Transformer Residual Connections},
author={Mike Heddes and Adel Javanmard and Kyriakos Axiotis and Gang Fu and Mohammadhossein Bateni and Vahab Mirrokni},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=j3JBfFnGYh}
}

@inproceedings{Srivastava2015highway,
 author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J\"{u}rgen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Training Very Deep Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf},
 volume = {28},
 year = {2015}
}

@misc{xie2023residualtransformerdualresidual,
      title={ResiDual: Transformer with Dual Residual Connections}, 
      author={Shufang Xie and Huishuai Zhang and Junliang Guo and Xu Tan and Jiang Bian and Hany Hassan Awadalla and Arul Menezes and Tao Qin and Rui Yan},
      year={2023},
      eprint={2304.14802},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.14802}, 
}

@inproceedings{
fang2023crosslayer,
title={Cross-Layer Retrospective Retrieving via Layer Attention},
author={Yanwen Fang and Yuxi CAI and Jintai Chen and Jingyu Zhao and Guangjian Tian and Guodong Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=pvgEL1yS3Ql}
}

@inproceedings{chai-etal-2020-highway,
    title = "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks",
    author = "Chai, Yekun  and
      Jin, Shuo  and
      Hou, Xinwen",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.616/",
    doi = "10.18653/v1/2020.acl-main.616",
    pages = "6887--6900",
    abstract = "Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process."
}

@article{sinkhorn1967concerning,
  title={Concerning nonnegative matrices and doubly stochastic matrices},
  author={Sinkhorn, Richard and Knopp, Paul},
  journal={Pacific Journal of Mathematics},
  volume={21},
  number={2},
  pages={343--348},
  year={1967},
}

@article{mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{bbh,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}


@inproceedings{drop,
  author       = {Dheeru Dua and
                  Yizhong Wang and
                  Pradeep Dasigi and
                  Gabriel Stanovsky and
                  Sameer Singh and
                  Matt Gardner},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{DROP:} {A} Reading Comprehension Benchmark Requiring Discrete Reasoning
                  Over Paragraphs},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {2368--2378},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1246},
  doi          = {10.18653/V1/N19-1246},
  timestamp    = {Fri, 06 Aug 2021 00:41:31 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DuaWDSS019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{hellaswag,
  author       = {Rowan Zellers and
                  Ari Holtzman and
                  Yonatan Bisk and
                  Ali Farhadi and
                  Yejin Choi},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {{HellaSwag}: Can a Machine Really Finish Your Sentence?},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {4791--4800},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1472},
  doi          = {10.18653/v1/p19-1472},
  timestamp    = {Sat, 29 Apr 2023 10:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZellersHBFC19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{piqa,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {7432--7439},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/aaai.v34i05.6239},
  timestamp    = {Mon, 04 Sep 2023 16:50:23 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study.",
}

@article{chen2021evaluating,
  title        = {Evaluating Large Language Models Trained on Code},
  author       = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and de Oliveira Pinto, Henrique Ponde and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Chen, Jonathan and Sigler, Eric and Ziegler, Daniel and Stiennon, Nisan and Wu, Jeffrey and Radford, Alec and Amodei, Dario and Sutskever, Ilya},
  journal      = {arXiv preprint arXiv:2107.03374},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03374}
}

@article{austin2021program,
  title        = {Program Synthesis with Large Language Models},
  author       = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal      = {arXiv preprint arXiv:2108.07732},
  year         = {2021},
  url          = {https://arxiv.org/abs/2108.07732}
}

@article{hendrycks2021apps,
  title        = {Measuring Coding Challenge Competence With {APPS}},
  author       = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
  journal      = {arXiv preprint arXiv:2105.09938},
  year         = {2021},
  url          = {https://arxiv.org/abs/2105.09938}
}

@article{jain2024livecodebench,
  title        = {LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code},
  author       = {Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal      = {arXiv preprint arXiv:2403.07974},
  year         = {2024},
  url          = {https://arxiv.org/abs/2403.07974}
}

@article{liu2023repobench,
  title        = {{RepoBench}: Benchmarking Repository-Level Code Auto-Completion Systems},
  author       = {Liu, Tianyang and Xu, Canwen and McAuley, Julian},
  journal      = {arXiv preprint arXiv:2306.03091},
  year         = {2023},
  url          = {https://arxiv.org/abs/2306.03091}
}

@article{jimenez2023swebench,
  title        = {{SWE}-bench: Can Language Models Resolve Real-World {GitHub} Issues?},
  author       = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and others},
  journal      = {arXiv preprint arXiv:2310.06770},
  year         = {2023},
  url          = {https://arxiv.org/abs/2310.06770}
}

@misc{fu2025automaticallybenchmarkingllmcode,
      title={Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation}, 
      author={Lingyue Fu and Bolun Zhang and Hao Guan and Yaoming Zhu and Lin Qiu and Weiwen Liu and Xuezhi Cao and Xunliang Cai and Weinan Zhang and Yong Yu},
      year={2025},
      eprint={2510.24358},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2510.24358}, 
}

@article{aleithan2024swebenchplus,
  title        = {{SWE}-Bench+: Enhanced Coding Benchmark for {LLM}s},
  author       = {Aleithan, Reem and Xue, Haoran and Mohajer, Mohammad Mahdi and Nnorom, Elijah and Uddin, Gias and Wang, Song},
  journal      = {arXiv preprint arXiv:2410.06992},
  year         = {2024},
  url          = {https://arxiv.org/abs/2410.06992}
}

@misc{deng2025swebenchpro,
  title        = {{SWE}-{Bench} Pro: Can {AI} Agents Solve Long-Horizon Software Engineering Tasks?},
  author       = {Deng, Xiang and Da, Jeff and Pan, Edwin and others},
  year         = {2025},
  eprint       = {2509.16941},
  archivePrefix= {arXiv},
  primaryClass = {cs.SE},
  doi          = {10.48550/arXiv.2509.16941},
  url          = {https://arxiv.org/abs/2509.16941}
}

@misc{jiang2025ossbenchbenchmarkgeneratorcoding,
      title={OSS-Bench: Benchmark Generator for Coding LLMs}, 
      author={Yuancheng Jiang and Roland Yap and Zhenkai Liang},
      year={2025},
      eprint={2505.12331},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2505.12331}, 
}

@misc{thai2026sweevobenchmarkingcodingagents,
      title={SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios}, 
      author={Minh V. T. Thai and Tue Le and Dung Nguyen Manh and Huy Phan Nhat and Nghi D. Q. Bui},
      year={2025},
      eprint={2512.18470},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2512.18470}, 
}

@misc{yang2024sweagent,
  title        = {{SWE}-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author       = {Yang, John and Jimenez, Carlos E. and Wettig, Alexander and others},
  year         = {2024},
  eprint       = {2405.15793},
  archivePrefix= {arXiv},
  primaryClass = {cs.SE},
  url          = {https://arxiv.org/abs/2405.15793}
}

@misc{liu2023evalplus,
  title        = {Is Your Code Generated by {ChatGPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  author       = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  year         = {2023},
  eprint       = {2305.01210},
  archivePrefix= {arXiv},
  primaryClass = {cs.SE},
  url          = {https://arxiv.org/abs/2305.01210}
}

@misc{cassano2022multiplE,
  title        = {{MultiPL-E}: A Scalable and Extensible Approach to Benchmarking Neural Code Generation},
  author       = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and others},
  year         = {2022},
  eprint       = {2208.08227},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2208.08227}
}

@article{liang2022helm,
  title        = {Holistic Evaluation of Language Models},
  author       = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal      = {arXiv preprint arXiv:2211.09110},
  year         = {2022},
  url          = {https://arxiv.org/abs/2211.09110}
}

@article{srivastava2022bigbench,
  title        = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author       = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and others},
  journal      = {arXiv preprint arXiv:2206.04615},
  year         = {2022},
  url          = {https://arxiv.org/abs/2206.04615}
}

@misc{moonbit,
  title        = {{MoonBit} Programming Language},
  author       = {{MoonBit Team}},
  howpublished = {\url{https://www.moonbitlang.com/}},
  year         = {2025},
}

@misc{yaml12,
  title        = {{YAML} Ain't Markup Language ({YAML}) Version 1.2},
  author       = {Ben-Kiki, Oren and Evans, Clark and Ingerson, Brian},
  howpublished = {\url{https://yaml.org/spec/1.2/spec.html}},
  year         = {2009},
}

@misc{toml10,
  title        = {{TOML} v1.0.0},
  author       = {Preston-Werner, Tom},
  howpublished = {\url{https://toml.io/en/v1.0.0}},
  year         = {2021},
}

@misc{rfc4180,
  author       = {Shafranovich, Y.},
  title        = {Common Format and {MIME} Type for Comma-Separated Values ({CSV}) Files},
  howpublished = {\url{https://www.rfc-editor.org/rfc/rfc4180}},
  year         = {2005},
}

@misc{whatwg-html,
  title        = {{HTML} Living Standard},
  author       = {{WHATWG}},
  howpublished = {\url{https://html.spec.whatwg.org/}},
  year         = {2025},
}

@misc{whatwg-url,
  title        = {{URL} Living Standard},
  author       = {{WHATWG}},
  howpublished = {\url{https://url.spec.whatwg.org/}},
  year         = {2025},
}

@misc{xml10,
  title        = {Extensible Markup Language ({XML}) 1.0 (Fifth Edition)},
  author       = {{W3C}},
  howpublished = {\url{https://www.w3.org/TR/xml/}},
  year         = {2008},
}

@misc{xml-namespaces,
  title        = {Namespaces in {XML} 1.0 (Third Edition)},
  author       = {{W3C}},
  howpublished = {\url{https://www.w3.org/TR/xml-names/}},
  year         = {2009},
}

@misc{rfc3986,
  author       = {Berners-Lee, Tim and Fielding, Roy and Masinter, Larry},
  title        = {Uniform Resource Identifier ({URI}): Generic Syntax},
  howpublished = {\url{https://www.rfc-editor.org/rfc/rfc3986}},
  year         = {2005},
}

@misc{wasm-core,
  title        = {WebAssembly Core Specification},
  author       = {{W3C WebAssembly Working Group}},
  howpublished = {\url{https://www.w3.org/TR/wasm-core-1/}},
  year         = {2019},
}

@misc{appnote-zip,
  title        = {{.ZIP} File Format Specification},
  author       = {{PKWARE}},
  howpublished = {\url{https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT}},
  year         = {2024},
}

@misc{sweagirepo,
  title        = {{SWE-AGI}: An Autonomous Software Engineering Benchmark from Explicit Specifications},
  author       = {{MoonBit Team}},
  howpublished = {\url{https://github.com/moonbitlang/SWE-AGI}},
  year         = {2025},
}

@misc{pug,
  title        = {Pug Template Engine Documentation},
  author       = {{Pug Team}},
  howpublished = {\url{https://pugjs.org/}},
  year         = {2025},
}

@misc{webidl,
  title        = {Web IDL Standard},
  author       = {{W3C}},
  howpublished = {\url{https://webidl.spec.whatwg.org/}},
  year         = {2025},
}

@misc{ecma262,
  title        = {{ECMA}-262: ECMAScript Language Specification},
  author       = {{ECMA International}},
  howpublished = {\url{https://tc39.es/ecma262/}},
  year         = {2025},
}

@misc{quickjs,
  title        = {QuickJS JavaScript Engine},
  author       = {Bellard, Fabrice},
  howpublished = {\url{https://bellard.org/quickjs/}},
  year         = {2024},
}

@misc{lua54,
  title        = {The {Lua} 5.4 Reference Manual},
  author       = {{Lua.org}},
  howpublished = {\url{https://www.lua.org/manual/5.4/}},
  year         = {2025},
}

@misc{python-ref,
  title        = {The Python Language Reference},
  author       = {{Python Software Foundation}},
  howpublished = {\url{https://docs.python.org/3/reference/}},
  year         = {2025},
}

@misc{jq,
  title        = {jq Manual},
  author       = {{jq contributors}},
  howpublished = {\url{https://jqlang.github.io/jq/manual/}},
  year         = {2025},
}

@misc{protobuf-encoding,
  title        = {Protocol Buffers Encoding},
  author       = {{Google}},
  howpublished = {\url{https://protobuf.dev/programming-guides/encoding/}},
  year         = {2025},
}

@misc{capnp-encoding,
  title        = {Cap'n Proto Encoding},
  author       = {{Cap'n Proto Authors}},
  howpublished = {\url{https://capnproto.org/encoding.html}},
  year         = {2024},
}

@misc{elf-abi,
  title        = {Executable and Linking Format ({ELF}) Specification},
  author       = {{Linux Foundation}},
  howpublished = {\url{https://refspecs.linuxfoundation.org/elf/gabi4+/contents.html}},
  year         = {2024},
}

@misc{macho,
  title        = {Mach-O Runtime Architecture},
  author       = {{Apple}},
  howpublished = {\url{https://developer.apple.com/library/archive/documentation/DeveloperTools/Conceptual/MachOTopics/0-Introduction/introduction.html}},
  year         = {2024},
}

@misc{git-objects,
  title        = {Git Internals: Git Objects},
  author       = {{Git Project}},
  howpublished = {\url{https://git-scm.com/book/en/v2/Git-Internals-Git-Objects}},
  year         = {2025},
}

@misc{thomas2026darkflow,
  title        = {Breaking the Spell of Vibe Coding},
  author       = {Thomas, Rachel},
  howpublished = {\url{https://www.fast.ai/posts/2026-01-28-dark-flow/}},
  year         = {2026},
}

@misc{c99,
  title        = {ISO C99 Overview},
  author       = {{ISO/IEC}},
  howpublished = {\url{https://www.iso.org/standard/29237.html}},
  year         = {1999},
}

@article{openai2023gpt4,
  title        = {{GPT}-4 Technical Report},
  author       = {{OpenAI}},
  journal      = {arXiv preprint arXiv:2303.08774},
  year         = {2023},
  url          = {https://arxiv.org/abs/2303.08774}
}

@article{openai2025gpt5,
  title        = {{OpenAI} {GPT}-5 System Card},
  author       = {{OpenAI}},
  journal      = {arXiv preprint arXiv:2601.03267},
  year         = {2025},
  url          = {https://arxiv.org/abs/2601.03267}
}

@article{google2023gemini,
  title        = {Gemini: A Family of Highly Capable Multimodal Models},
  author       = {Team, Gemini and others},
  journal      = {arXiv preprint arXiv:2312.11805},
  year         = {2023},
  url          = {https://arxiv.org/abs/2312.11805}
}

@article{google2025gemini25,
  title        = {Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities},
  author       = {{Gemini Team} and others},
  journal      = {arXiv preprint arXiv:2507.06261},
  year         = {2025},
  url          = {https://arxiv.org/abs/2507.06261}
}

@article{google2024gemini15,
  title        = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author       = {{Gemini Team} and others},
  journal      = {arXiv preprint arXiv:2403.05530},
  year         = {2024},
  url          = {https://arxiv.org/abs/2403.05530}
}

@misc{anthropic2024claude3,
  title        = {Claude 3 Model Card},
  author       = {{Anthropic}},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-family}},
  year         = {2024},
}

@misc{anthropic2025claude4,
  title        = {Claude Sonnet 4.5},
  author       = {{Anthropic}},
  howpublished = {\url{https://www.anthropic.com/news/claude-sonnet-4-5}},
  year         = {2025},
}

@article{qwen2025qwen3,
  title        = {Qwen3 Technical Report},
  author       = {{Qwen Team} and others},
  journal      = {arXiv preprint arXiv:2505.09388},
  year         = {2025},
  url          = {https://arxiv.org/abs/2505.09388}
}

@article{qwen2024qwen25,
  title        = {Qwen2.5 Technical Report},
  author       = {{Qwen Team} and others},
  journal      = {arXiv preprint arXiv:2412.15115},
  year         = {2024},
  url          = {https://arxiv.org/abs/2412.15115}
}

@article{kimi2025k15,
  title        = {Kimi k1.5: Scaling Reinforcement Learning with {LLM}s},
  author       = {{Kimi Team} and others},
  journal      = {arXiv preprint arXiv:2501.12599},
  year         = {2025},
  url          = {https://arxiv.org/abs/2501.12599}
}

@article{kimi2025k2,
  title        = {Kimi K2: Open Agentic Intelligence},
  author       = {{Kimi Team} and others},
  journal      = {arXiv preprint arXiv:2507.20534},
  year         = {2025},
  url          = {https://arxiv.org/abs/2507.20534}
}

@article{deepseek2025v32,
  title        = {DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models},
  author       = {{DeepSeek Team} and others},
  journal      = {arXiv preprint arXiv:2512.02556},
  year         = {2025},
  url          = {https://arxiv.org/abs/2512.02556}
}

@article{step2025deepresearch,
  title        = {Step-DeepResearch Technical Report},
  author       = {Chen Hu and others},
  journal      = {arXiv preprint arXiv:2512.20491},
  year         = {2025},
  url          = {https://arxiv.org/abs/2512.20491}
}

@misc{liu2023agentbench,
  title         = {AgentBench: Evaluating {LLM}s as Agents},
  author        = {Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
  year          = {2023},
  eprint        = {2308.03688},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2308.03688}
}

@misc{zhou2023webarena,
  title         = {{WebArena}: A Realistic Web Environment for Building Autonomous Agents},
  author        = {Shuyan Zhou and Frank F. Xu and Hao Zhu and Xuhui Zhou and Robert Lo and Abishek Sridhar and Xianyi Cheng and Tianyue Ou and Yonatan Bisk and Daniel Fried and Uri Alon and Graham Neubig},
  year          = {2023},
  eprint        = {2307.13854},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2307.13854}
}

@misc{yao2022react,
  title         = {{ReAct}: Synergizing Reasoning and Acting in Language Models},
  author        = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year          = {2022},
  eprint        = {2210.03629},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2210.03629}
}

@misc{schick2023toolformer,
  title         = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  author        = {Schick, Timo and Dwivedi-Yu, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola},
  year          = {2023},
  eprint        = {2302.04761},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  url           = {https://arxiv.org/abs/2302.04761}
}

@misc{shinn2023reflexion,
  title         = {Reflexion: Language Agents with Verbal Reinforcement Learning},
  author        = {Shinn, Noah and Labash, Beck and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  year          = {2023},
  eprint        = {2303.11366},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2303.11366}
}

@misc{qin2023toollm,
  title         = {{ToolLLM}: Facilitating Large Language Models to Master 16000+ Real-world {API}s},
  author        = {Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
  year          = {2023},
  eprint        = {2307.16789},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url           = {https://arxiv.org/abs/2307.16789}
}

@misc{tbench2025terminalbench,
  title  = {Terminal-Bench: A Benchmark for {AI} Agents in Terminal Environments},
  author = {{The Terminal-Bench Team}},
  year   = {2025},
  month  = {Apr},
  url    = {https://github.com/laude-institute/terminal-bench}
}
